# -*- coding: utf-8 -*-
"""gpt-oss-(20B)-Fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # We're installing the latest Torch, Triton, OpenAI's Triton kernels, Transformers and Unsloth!
# !pip install --upgrade -qqq uv
# try: import numpy; get_numpy = f"numpy=={numpy.__version__}"
# except: get_numpy = "numpy"
# !uv pip install -qqq \
#     "torch>=2.8.0" "triton>=3.4.0" {get_numpy} torchvision bitsandbytes "transformers>=4.55.3" \
#     "unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo" \
#     "unsloth[base] @ git+https://github.com/unslothai/unsloth" \
#     git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels
# !uv pip install transformers==4.55.4
# !uv pip install --no-deps trl==0.22.2

"""### Unsloth

We're about to demonstrate the power of the new OpenAI GPT-OSS 20B model through a finetuning example. To use our `MXFP4` inference example, use this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) instead.
"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 1024
dtype = None

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/gpt-oss-20b-unsloth-bnb-4bit", # 20B model using bitsandbytes 4bit quantization
    "unsloth/gpt-oss-120b-unsloth-bnb-4bit",
    "unsloth/gpt-oss-20b", # 20B model using MXFP4 format
    "unsloth/gpt-oss-120b",
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/gpt-oss-20b",
    dtype = dtype, # None for auto detection
    max_seq_length = max_seq_length, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = "hf_...", # use one if using gated models
)

"""We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."""

model = FastLanguageModel.get_peft_model(
    model,
    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

import pandas as pd
df = pd.read_csv('/content/ticketMarche_test.csv')

df

import re

def predict_attendance(event):
    messages = [
      {
          "role": "user",
          "content": f"""
          Act as an event organizer that is trying to estimate
          how much people will attend an event based on the following events:
          Event: {event['event_name']}
          Venue: {event['venue_name']}
          Date: {event['date_parsed']}
          Time: {event['Time']}
          City: {event['city']}
        Take into consideration that the artist name is in the event name so you can use their popularity as an extra field for estiamtion.
        Respond ONLY with a single integer, inside <|final|> tags like this:
        THE OUTPUT MUST HAVE THE NUMBER IN THE FOLLOWING FORMAT <|final|>predicted number<|end|>
        Do NOT include <|analysis|>.
          """
      }
  ]

    # inputs = tokenizer(prompt, return_tensors="pt")
    inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
    return_dict = True,
    reasoning_effort = "high", # **NEW!** Set reasoning effort to low, medium or high
).to(model.device)

    gen_tokens = model.generate(
        **inputs,
        max_new_tokens=1024*2,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id

    )

    # keep only the new tokens (skip the prompt)
    new_tokens = gen_tokens[0][inputs["input_ids"].shape[-1]:]
    gen_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
    match_ = re.search(r"<\|final\|>(\d+)(<\|end\|>)?", gen_text)
    event['estimated_attendance'] = int(match_.group(1))
    return (match_, gen_text, event)

match_, gen_text, event = predict_attendance(df.iloc[0,:])

gen_text

match_.group(1)

event

